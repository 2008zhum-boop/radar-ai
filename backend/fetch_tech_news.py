"""
æ–°å¢å‡½æ•°ï¼šfetch_tech_news_by_tags
åŠŸèƒ½ï¼šè·å–"ç§‘æŠ€"åˆ†ç±»ä¸‹çš„äºŒçº§æ ‡ç­¾ï¼Œå¹¶ç”¨è¿™äº›æ ‡ç­¾å»Googleæœç´¢æœ€è¿‘6å°æ—¶çš„æ–°é—»
"""
import sqlite3
import urllib.parse
import requests
from bs4 import BeautifulSoup
import random

DB_FILE = "radar_data.db"
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
}

def fetch_tech_news_by_tags():
    """
    1. ä»æ•°æ®åº“è·å–"ç§‘æŠ€"åˆ†ç±»ä¸‹çš„æ‰€æœ‰äºŒçº§æ ‡ç­¾
    2. ç”¨æ ‡ç­¾ä½œä¸ºå…³é”®è¯æœç´¢Google News (æœ€è¿‘6å°æ—¶)
    3. æŠ“å–æ–°é—»è¯¦æƒ…å¹¶å…¥åº“
    """
    from radar_weibo import enrich_news_full
    
    conn = sqlite3.connect(DB_FILE)
    c = conn.cursor()
    
    items = []
    seen_titles = set()
    
    # 1. è·å–"ç§‘æŠ€"åˆ†ç±»çš„ ID
    c.execute("SELECT id FROM tags WHERE name='ç§‘æŠ€' AND tag_type='CATEGORY'")
    tech_category = c.fetchone()
    
    if not tech_category:
        print("âš ï¸ Warning: 'ç§‘æŠ€' category not found in database.")
        conn.close()
        return []
    
    tech_id = tech_category[0]
    
    # 2. è·å–ç§‘æŠ€åˆ†ç±»ä¸‹çš„æ‰€æœ‰äºŒçº§æ ‡ç­¾
    c.execute("""
        SELECT t.name 
        FROM tags t 
        JOIN tag_relations tr ON t.id = tr.child_id 
        WHERE tr.parent_id = ?
    """, (tech_id,))
    
    tech_tags = [row[0] for row in c.fetchall()]
    conn.close()
    
    if not tech_tags:
        print("âš ï¸ Warning: No secondary tags found under 'ç§‘æŠ€' category.")
        return []
    
    print(f"ğŸ” Fetching tech news for {len(tech_tags)} tags: {tech_tags[:5]}...")
    
    # 3. åˆ†æ‰¹æŸ¥è¯¢ (é¿å…URLè¿‡é•¿)
    chunk_size = 5
    chunks = [tech_tags[i:i + chunk_size] for i in range(0, len(tech_tags), chunk_size)]
    
    for chunk in chunks:
        try:
            # æ„å»ºæŸ¥è¯¢: "(Tag1 OR Tag2 OR ...) when:6h"
            query_str = "(" + " OR ".join(chunk) + ")"
            encoded_q = urllib.parse.quote(query_str)
            
            # Google News RSS (æœ€è¿‘6å°æ—¶)
            rss_url = f"https://news.google.com/rss/search?q={encoded_q}+when:6h&hl=zh-CN&gl=CN&ceid=CN:zh-CN"
            
            resp = requests.get(rss_url, headers=HEADERS, timeout=10)
            if resp.status_code != 200:
                continue
            
            soup = BeautifulSoup(resp.content, 'xml')
            entries = soup.find_all('item')
            
            for entry in entries[:8]:  # æ¯æ‰¹å–å‰8æ¡
                title = entry.title.text if entry.title else ""
                
                if not title or title in seen_titles:
                    continue
                seen_titles.add(title)
                
                link = entry.link.text if entry.link else ""
                desc = entry.description.text if entry.description else ""
                pub_date = entry.pubDate.text if entry.pubDate else ""
                
                # æ¸…ç†æè¿°
                desc_text = BeautifulSoup(desc, "lxml").get_text().strip()
                
                items.append({
                    "rank": 999,
                    "title": title,
                    "heat": random.randint(10000, 500000),
                    "label": "ç§‘",
                    "category": "ç§‘æŠ€",
                    "tags": chunk[:3],
                    "url": link,
                    "raw_summary_context": desc_text[:200],
                    "pub_date": pub_date,
                    "source": "å…¨ç½‘ç›‘æ§-ç§‘æŠ€"
                })
                
        except Exception as e:
            print(f"  Error fetching tech news chunk: {e}")
    
    print(f"âœ… Found {len(items)} tech news items. Enriching with full content...")
    
    # 4. è°ƒç”¨å¢å¼ºå‡½æ•°æŠ“å–å®Œæ•´å†…å®¹
    # enrich_news_full ä¼šæŠ“å–ï¼šæ ‡é¢˜ã€æ‘˜è¦ã€æ­£æ–‡ã€ä½œè€…ã€å‘å¸ƒæ—¶é—´ã€URL
    # å¦‚æœæ‘˜è¦ä¸ºç©ºï¼Œä¼šè°ƒç”¨AIç”Ÿæˆ
    enriched = enrich_news_full(items)
    
    return enriched


if __name__ == "__main__":
    # æµ‹è¯•
    results = fetch_tech_news_by_tags()
    print(f"\nğŸ“° Successfully fetched {len(results)} tech news articles")
    if results:
        print(f"Sample: {results[0]['title']}")
