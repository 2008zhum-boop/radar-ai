import requests
from bs4 import BeautifulSoup
import time
import random
import json
import sqlite3
import hashlib
import os
from datetime import datetime, timedelta
from urllib.parse import quote
from concurrent.futures import ThreadPoolExecutor

# å¼•ç”¨ AI
from radar_ai import generate_news_summary

DB_FILE = "radar_data.db"
CACHE_EXPIRE_SECONDS = 600

# âœ… ç„Šæ­»ä»£ç†é…ç½® (ä¸ radar_ai ä¿æŒä¸€è‡´)
PROXY_URL = "socks5h://127.0.0.1:9091"

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
}

def init_db():
    conn = sqlite3.connect(DB_FILE, timeout=30.0)
    c = conn.cursor()
    c.execute('''CREATE TABLE IF NOT EXISTS hot_cache (source text PRIMARY KEY, data text, updated_at real)''')
    conn.commit()
    conn.close()

init_db()

# === æ ¸å¿ƒç®—æ³•ï¼šçƒ­åº¦è®¡ç®— ===
def calculate_heat(source_name, score, rank):
    base = {"Googleæ–°é—»": 800000, "36æ°ª": 500000, "GitHub": 200000}.get(source_name, 300000)
    rank_drop = max(0.4, 1 - (rank * 0.03))
    score_boost = (score / 60) ** 2 
    return int(base * rank_drop * score_boost * random.uniform(0.9, 1.1))

# === æ™ºèƒ½å…œåº•ï¼šåŸºç¡€æƒ…æ„Ÿ ===
def calculate_fallback_sentiment(title):
    pos_keywords = ["çªç ´", "å¤§æ¶¨", "æ–°é«˜", "å‘å¸ƒ", "æˆåŠŸ", "å¢é•¿", "è·æ‰¹", "é¦–å‘", "åˆ©å¥½"]
    neg_keywords = ["è£å‘˜", "æš´è·Œ", "äºæŸ", "è°ƒæŸ¥", "ç½šæ¬¾", "è­¦ç¤º", "ä¸‹è·Œ", "å¤±è´¥", "æ¼æ´"]
    score = 0
    for k in pos_keywords:
        if k in title: score += 1
    for k in neg_keywords:
        if k in title: score -= 1
        
    if score > 0: return {"positive": 80, "neutral": 15, "negative": 5}
    elif score < 0: return {"positive": 5, "neutral": 15, "negative": 80}
    else: return {"positive": 10, "neutral": 80, "negative": 10}

# === æ™ºèƒ½å…œåº•ï¼šç»†ç²’åº¦æƒ…ç»ª ===
def calculate_fallback_emotions(title):
    emotions = { "anxiety": 5, "anger": 5, "sadness": 5, "excitement": 5, "sarcasm": 5 }
    rules = [
        (["è£å‘˜", "åˆ¶è£", "æ‹…å¿§", "é£é™©", "è­¦å‘Š", "å»¶æœŸ", "æš´é›·"], "anxiety", 60),
        (["è¢«æŸ¥", "ç½šæ¬¾", "ä¾µæƒ", "ä¸‘é—»", "é€ å‡", "æŠ—è®®", "åšç©º"], "anger", 70),
        (["é€ä¸–", "æš´è·Œ", "äºæŸ", "å¤±è´¥", "è…°æ–©", "æƒ¨æ·¡"], "sadness", 60),
        (["é¦–å‘", "çªç ´", "å¤§æ¶¨", "æ–°é«˜", "è·æ‰¹", "é‡ç£…", "é¥é¥é¢†å…ˆ"], "excitement", 80),
        (["åè½¬", "åƒç“œ", "æ‰“è„¸", "ç¦»è°±", "éœ‡æƒŠ", "è¾Ÿè°£"], "sarcasm", 50)
    ]
    for keywords, emo_key, score in rules:
        for k in keywords:
            if k in title:
                emotions[emo_key] = max(emotions[emo_key], score + random.randint(-10, 10))
    return emotions

# === æŠ“å–å·¥å…· ===
def fetch_page_content(url):
    if not url or "github" in url or "google" in url: return ""
    try:
        # æ­£æ–‡æŠ“å–æ ¹æ® URL å†³å®šæ˜¯å¦èµ°ä»£ç†
        use_proxy = "36kr.com" not in url 
        proxies = {"http": PROXY_URL, "https": PROXY_URL} if use_proxy else None
        
        resp = requests.get(url, headers=HEADERS, timeout=5, proxies=proxies)
        if resp.status_code == 200:
            soup = BeautifulSoup(resp.text, 'lxml')
            return "\n".join([p.get_text().strip() for p in soup.find_all('p') if len(p.get_text()) > 20])
    except: pass
    return ""

def enrich_items(items, source_name):
    print(f"[{source_name}] æŠ“å–æˆåŠŸ {len(items)} æ¡ï¼Œæ­£åœ¨è¿›è¡Œ AI åˆ†æ...")
    
    def process(item):
        item['source'] = source_name 
        
        try:
            content = fetch_page_content(item.get('url'))
            ai_data = generate_news_summary(item['title'], content)
        except Exception as e:
            # print(f"AI Error: {e}")
            ai_data = {} 

        fact = ai_data.get('fact') or item['title'] 
        score = ai_data.get('score', 60)
        category = ai_data.get('category', 'ç»¼åˆ')
        
        sentiment = ai_data.get('sentiment')
        if not isinstance(sentiment, dict):
             sentiment = calculate_fallback_sentiment(item['title'])
             
        emotions = ai_data.get('emotions')
        if not isinstance(emotions, dict):
            emotions = calculate_fallback_emotions(item['title'])

        item.update({
            'heat': calculate_heat(source_name, score, item.get('rank', 10)),
            'score': score,
            'trend': ai_data.get('trend', 'å¹³ç¨³'),
            'reason': ai_data.get('reason', ''),
            'category': category,
            'summary': ai_data,
            'fact': fact,
            'tags': ai_data.get('tags', []),
            'sentiment': sentiment,
            'emotions': emotions
        })
        return item

    with ThreadPoolExecutor(max_workers=5) as executor:
        result = list(executor.map(process, items))
    
    return result

def cache_and_save(source_name, items):
    if not items: 
        print(f"âš ï¸ [{source_name}] æŠ“å–ç»“æœä¸ºç©ºï¼Œè·³è¿‡ç¼“å­˜ã€‚")
        return []
    try:
        conn = sqlite3.connect(DB_FILE, timeout=30.0)
        c = conn.cursor()
        c.execute("REPLACE INTO hot_cache (source, data, updated_at) VALUES (?, ?, ?)", 
                  (source_name, json.dumps(items, ensure_ascii=False), time.time()))
        conn.commit()
        conn.close()
    except Exception as e: print(f"Cache Error: {e}")
    return items

def get_from_cache(source_name):
    try:
        conn = sqlite3.connect(DB_FILE, timeout=30.0)
        c = conn.cursor()
        c.execute("SELECT data, updated_at FROM hot_cache WHERE source=?", (source_name,))
        row = c.fetchone()
        conn.close()
        if row and (time.time() - row[1] < CACHE_EXPIRE_SECONDS):
            return json.loads(row[0])
    except: pass
    return None

# === ä¿¡æºæŠ“å– ===

def fetch_google():
    # âœ… å¼ºåˆ¶èµ°ä»£ç†
    proxies = {"http": PROXY_URL, "https": PROXY_URL}
    
    queries = ["AIå¤§æ¨¡å‹ DeepSeek OpenAI", "ç§‘æŠ€å·¨å¤´ è´¢æŠ¥ è£å‘˜", "æ–°èƒ½æº åä¸º å°ç±³"]
    items = []
    seen = set()
    print("ğŸš€ æ­£åœ¨æŠ“å–: Googleæ–°é—» (ä½¿ç”¨ä»£ç†)...")
    
    try:
        for q in queries:
            url = f"https://news.google.com/rss/search?q={quote(q)}+when:24h&hl=zh-CN&gl=CN&ceid=CN:zh-CN"
            resp = requests.get(url, headers=HEADERS, timeout=10, proxies=proxies)
            if resp.status_code != 200:
                print(f"âŒ Google è¯·æ±‚å¤±è´¥: {resp.status_code}")
                continue
                
            soup = BeautifulSoup(resp.content, 'xml')
            for entry in soup.find_all('item')[:5]:
                raw_title = entry.title.text
                if raw_title in seen: continue
                seen.add(raw_title)
                clean_title = raw_title
                if " - " in clean_title: clean_title = clean_title.rsplit(" - ", 1)[0].strip()
                items.append({"rank": len(items)+1, "title": clean_title, "url": entry.link.text, "source": "Googleæ–°é—»"})
    except Exception as e:
        print(f"âŒ Google æŠ“å–å¼‚å¸¸: {e}")
        
    return enrich_items(items, "Googleæ–°é—»")

def fetch_36kr():
    # âœ… å¼ºåˆ¶ä¸èµ°ä»£ç† (å›½å†…ç«™)
    proxies = None 
    
    items = []
    print("ğŸš€ æ­£åœ¨æŠ“å–: 36æ°ª (å›½å†…ç›´è¿)...")
    
    try:
        resp = requests.get("https://36kr.com/newsflashes", headers=HEADERS, timeout=5, proxies=proxies)
        if resp.status_code != 200:
            print(f"âŒ 36Kr è¯·æ±‚å¤±è´¥: {resp.status_code}")
        else:
            soup = BeautifulSoup(resp.text, 'lxml')
            for i, t in enumerate(soup.find_all('a', class_='item-title')[:15]): 
                items.append({"rank": i+1, "title": t.get_text().strip(), "url": "https://36kr.com"+t.get('href'), "source": "36æ°ª"})
    except Exception as e:
        print(f"âŒ 36Kr æŠ“å–å¼‚å¸¸: {e}")
        
    return enrich_items(items, "36æ°ª")

def fetch_github():
    # âœ… å¼ºåˆ¶èµ°ä»£ç†
    proxies = {"http": PROXY_URL, "https": PROXY_URL}
    
    items = []
    print("ğŸš€ æ­£åœ¨æŠ“å–: GitHub (ä½¿ç”¨ä»£ç†)...")
    
    try:
        url = "https://api.github.com/search/repositories?q=topic:ai+created:>2025-01-01&sort=stars&order=desc"
        resp = requests.get(url, headers=HEADERS, timeout=10, proxies=proxies)
        if resp.status_code != 200:
             print(f"âŒ GitHub è¯·æ±‚å¤±è´¥: {resp.status_code}")
        else:
            data = resp.json()
            for i, r in enumerate(data.get('items', [])[:8]):
                items.append({"rank": i+1, "title": f"GitHub: {r['name']}", "url": r['html_url'], "source": "GitHub"})
    except Exception as e:
        print(f"âŒ GitHub æŠ“å–å¼‚å¸¸: {e}")
        
    return enrich_items(items, "GitHub")

def get_weibo_hot_list(category="ç»¼åˆ"):
    all_data = {}
    # å®šä¹‰æº
    sources = [("Googleæ–°é—»", fetch_google), ("36æ°ª", fetch_36kr), ("GitHub", fetch_github)]
    
    for name, func in sources:
        # å…ˆè¯»ç¼“å­˜
        data = get_from_cache(name)
        
        # ç¼“å­˜æ²¡æ•°æ®ï¼Œè¿›è¡ŒæŠ“å–
        if not data:
            data = cache_and_save(name, func())
        
        # è¿‡æ»¤
        if data:
            if category == "ç»¼åˆ": all_data[name] = data
            else:
                filtered = [x for x in data if x.get('category') == category]
                if filtered: all_data[name] = filtered
                
    return all_data

def search_news_content(keyword): return [] 
def sync_hot_to_mentions(items, source): pass