import requests
from bs4 import BeautifulSoup
import time
import random
import json
import sqlite3
import os

# === æ•°æ®åº“é…ç½® ===
DB_FILE = "radar_data.db"
CACHE_EXPIRE_SECONDS = 3600  # æ”¹ä¸º 1 å°æ—¶è¿‡æœŸï¼Œä½“éªŒæ›´å¥½

# åˆå§‹åŒ–æ•°æ®åº“
def init_db():
    conn = sqlite3.connect(DB_FILE)
    c = conn.cursor()
    # åˆ›å»ºä¸€ä¸ªè¡¨ï¼škey(æ¥æº), data(JSONæ•°æ®), updated_at(æ—¶é—´æˆ³)
    c.execute('''CREATE TABLE IF NOT EXISTS hot_cache
                 (source text PRIMARY KEY, data text, updated_at real)''')
    conn.commit()
    conn.close()

# å¯åŠ¨æ—¶åˆå§‹åŒ–ä¸€æ¬¡
init_db()

# === æ•°æ®åº“è¯»å†™å‡½æ•° ===
def get_db_cache(source):
    """å°è¯•ä»æ•°æ®åº“è¯»å–æœ‰æ•ˆç¼“å­˜"""
    try:
        conn = sqlite3.connect(DB_FILE)
        c = conn.cursor()
        c.execute("SELECT data, updated_at FROM hot_cache WHERE source=?", (source,))
        row = c.fetchone()
        conn.close()

        if row:
            data_json, updated_at = row
            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            if time.time() - updated_at < CACHE_EXPIRE_SECONDS:
                print(f"[{source}] âš¡ï¸ å‘½ä¸­æ•°æ®åº“ç¼“å­˜ (æ— éœ€è”ç½‘)")
                return json.loads(data_json)
            else:
                print(f"[{source}] âš ï¸ ç¼“å­˜å·²è¿‡æœŸï¼Œå‡†å¤‡é‡æ–°æŠ“å–...")
        return None
    except Exception as e:
        print(f"è¯»ç¼“å­˜å‡ºé”™: {e}")
        return None

def set_db_cache(source, data):
    """å†™å…¥æ•°æ®åº“"""
    try:
        conn = sqlite3.connect(DB_FILE)
        c = conn.cursor()
        # æ’å…¥æˆ–æ›´æ–° (REPLACE INTO)
        c.execute("REPLACE INTO hot_cache (source, data, updated_at) VALUES (?, ?, ?)", 
                  (source, json.dumps(data, ensure_ascii=False), time.time()))
        conn.commit()
        conn.close()
        print(f"[{source}] âœ… æ•°æ®å·²å­˜å…¥æ•°æ®åº“")
    except Exception as e:
        print(f"å†™ç¼“å­˜å‡ºé”™: {e}")

# === ä¼ªè£…å¤´ ===
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# === 1. å¾®åšçƒ­æœ ===
def fetch_weibo():
    try:
        url = "https://weibo.com/ajax/side/hotSearch"
        resp = requests.get(url, headers=HEADERS, timeout=5)
        data = resp.json()
        items = []
        realtime_list = data.get('data', {}).get('realtime', [])
        for i, item in enumerate(realtime_list[:20]):
            if item.get('is_ad'): continue
            title = item.get('word_scheme', item.get('word'))
            label = item.get('icon_desc', '')
            items.append({
                "rank": i + 1,
                "title": title,
                "heat": item.get('num', 0),
                "label": label[:1],
                "summary": f"å¾®åšå®æ—¶çƒ­åº¦ï¼š{item.get('num', 0)}",
                "source": "å¾®åšçƒ­æœ"
            })
        return items
    except Exception as e:
        print(f"å¾®åšæŠ“å–å¤±è´¥: {e}")
        return []

# === 2. 36æ°ªå¿«è®¯ ===
def fetch_36kr():
    try:
        url = "https://36kr.com/newsflashes"
        resp = requests.get(url, headers=HEADERS, timeout=5)
        soup = BeautifulSoup(resp.text, 'lxml')
        items = []
        news_list = soup.find_all('div', class_='newsflash-item')
        for i, news in enumerate(news_list[:15]):
            title_tag = news.find('a', class_='item-title')
            desc_tag = news.find('div', class_='item-desc')
            if title_tag:
                items.append({
                    "rank": i + 1,
                    "title": title_tag.get_text().strip(),
                    "heat": random.randint(50000, 200000),
                    "label": "å¿«è®¯" if i < 3 else "",
                    "summary": desc_tag.get_text().strip() if desc_tag else "",
                    "source": "36æ°ª"
                })
        return items
    except Exception as e:
        print(f"36æ°ªæŠ“å–å¤±è´¥: {e}")
        return []

# === 3. ç™¾åº¦é£äº‘æ¦œ ===
def fetch_baidu():
    try:
        url = "https://top.baidu.com/board?tab=realtime"
        resp = requests.get(url, headers=HEADERS, timeout=5)
        soup = BeautifulSoup(resp.text, 'lxml')
        items = []
        rows = soup.find_all('div', class_='category-wrap_iQLoo')
        for i, row in enumerate(rows[:15]):
            title_div = row.find('div', class_='c-single-text-ellipsis')
            heat_div = row.find('div', class_='hot-index_1Bl1a')
            desc_div = row.find('div', class_='hot-desc_1m_jR')
            if title_div:
                try: heat_val = int(heat_div.get_text().strip())
                except: heat_val = 0
                items.append({
                    "rank": i + 1,
                    "title": title_div.get_text().strip(),
                    "heat": heat_val,
                    "label": "çƒ­" if i < 3 else "",
                    "summary": desc_div.get_text().strip() if desc_div else "ç™¾åº¦å®æ—¶æœç´¢çƒ­ç‚¹",
                    "source": "ç™¾åº¦é£äº‘æ¦œ"
                })
        return items
    except Exception as e:
        print(f"ç™¾åº¦æŠ“å–å¤±è´¥: {e}")
        return []

# === 4. é’›åª’ä½“ ===
def fetch_tmt():
    try:
        url = "https://www.tmtpost.com/"
        resp = requests.get(url, headers=HEADERS, timeout=5)
        soup = BeautifulSoup(resp.text, 'lxml')
        items = []
        posts = soup.find_all('h3', class_='post_title')
        for i, post in enumerate(posts[:15]):
            link = post.find('a')
            if link:
                items.append({
                    "rank": i + 1,
                    "title": link.get_text().strip(),
                    "heat": random.randint(10000, 80000),
                    "label": "TMT",
                    "summary": "é’›åª’ä½“å‰æ²¿ç§‘æŠ€æŠ¥é“",
                    "source": "é’›åª’ä½“App"
                })
        return items
    except Exception as e:
        print(f"é’›åª’ä½“æŠ“å–å¤±è´¥: {e}")
        return []

# === é€šç”¨è·å–é€»è¾‘ ===
def get_data_with_cache(source_name, fetch_func):
    # 1. å…ˆæŸ¥åº“
    cached = get_db_cache(source_name)
    if cached:
        return cached
    
    # 2. æ²¡åº“æˆ–è¿‡æœŸï¼Œè”ç½‘æŠ“
    print(f"[{source_name}] ğŸŒ æ­£åœ¨è”ç½‘æŠ“å–...")
    data = fetch_func()
    
    # 3. å­˜åº“ (åªæœ‰æŠ“å–æˆåŠŸæ‰å­˜)
    if data:
        set_db_cache(source_name, data)
        return data
    else:
        # å¦‚æœè”ç½‘å¤±è´¥ï¼Œå°è¯•è¯»å–æ—§ç¼“å­˜ï¼ˆå³ä½¿è¿‡æœŸä¹Ÿæ¯”ç©ºç€å¼ºï¼‰
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œç›´æ¥è¿”å›ç©º
        return []

# === ä¸»å…¥å£ ===
def get_weibo_hot_list(category="ç»¼åˆ"):
    result = {}
    
    # å®šä¹‰ä»»åŠ¡æ¸…å•
    tasks = []
    if category in ["ç»¼åˆ", "æ–°æ¶ˆè´¹", "å¤§å¥åº·", "å‡ºæµ·"]:
        tasks.append(("å¾®åšçƒ­æœ", fetch_weibo))
    if category in ["ç»¼åˆ", "ç§‘æŠ€", "åˆ›æŠ•", "è´¢ç»"]:
        tasks.append(("36æ°ª", fetch_36kr))
    if category in ["ç»¼åˆ"]:
        tasks.append(("ç™¾åº¦é£äº‘æ¦œ", fetch_baidu))
    
    # é’›åª’ä½“ä»»ä½•æ—¶å€™éƒ½æœ‰
    tasks.append(("é’›åª’ä½“App", fetch_tmt))

    # æ‰§è¡Œä»»åŠ¡
    for source, func in tasks:
        data = get_data_with_cache(source, func)
        if data:
            result[source] = data

    return result