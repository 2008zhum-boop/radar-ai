import requests
from bs4 import BeautifulSoup
import time
import random
import json
import sqlite3
import os

# === æ•°æ®åº“é…ç½® ===
DB_FILE = "radar_data.db"
CACHE_EXPIRE_SECONDS = 3600  # 1 å°æ—¶ç¼“å­˜

# åˆå§‹åŒ–æ•°æ®åº“
def init_db():
    conn = sqlite3.connect(DB_FILE)
    c = conn.cursor()
    c.execute('''CREATE TABLE IF NOT EXISTS hot_cache
                 (source text PRIMARY KEY, data text, updated_at real)''')
    conn.commit()
    conn.close()

init_db()

# === æ•°æ®åº“è¯»å†™ ===
def get_db_cache(source):
    try:
        conn = sqlite3.connect(DB_FILE)
        c = conn.cursor()
        c.execute("SELECT data, updated_at FROM hot_cache WHERE source=?", (source,))
        row = c.fetchone()
        conn.close()

        if row:
            data_json, updated_at = row
            if time.time() - updated_at < CACHE_EXPIRE_SECONDS:
                print(f"[{source}] âš¡ï¸ å‘½ä¸­æ•°æ®åº“ç¼“å­˜")
                return json.loads(data_json)
            else:
                print(f"[{source}] âš ï¸ ç¼“å­˜å·²è¿‡æœŸï¼Œé‡æ–°æŠ“å–...")
        return None
    except Exception as e:
        print(f"è¯»ç¼“å­˜å‡ºé”™: {e}")
        return None

def set_db_cache(source, data):
    try:
        conn = sqlite3.connect(DB_FILE)
        c = conn.cursor()
        c.execute("REPLACE INTO hot_cache (source, data, updated_at) VALUES (?, ?, ?)", 
                  (source, json.dumps(data, ensure_ascii=False), time.time()))
        conn.commit()
        conn.close()
        print(f"[{source}] âœ… æ•°æ®å·²å­˜å…¥æ•°æ®åº“")
    except Exception as e:
        print(f"å†™ç¼“å­˜å‡ºé”™: {e}")

# === ä¼ªè£…å¤´ ===
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# === 1. å¾®åšçƒ­æœ ===
def fetch_weibo():
    try:
        url = "https://weibo.com/ajax/side/hotSearch"
        resp = requests.get(url, headers=HEADERS, timeout=5)
        data = resp.json()
        items = []
        realtime_list = data.get('data', {}).get('realtime', [])
        for i, item in enumerate(realtime_list[:20]):
            if item.get('is_ad'): continue
            title = item.get('word_scheme', item.get('word'))
            label = item.get('icon_desc', '')
            items.append({
                "rank": i + 1,
                "title": title,
                "heat": item.get('num', 0),
                "label": label[:1],
                "summary": f"å¾®åšå®æ—¶çƒ­åº¦ï¼š{item.get('num', 0)}",
                "source": "å¾®åšçƒ­æœ"
            })
        return items
    except Exception as e:
        print(f"å¾®åšæŠ“å–å¤±è´¥: {e}")
        return []

# === 2. å¤´æ¡å· (åŸ36æ°ªé€»è¾‘ï¼Œä»…æ”¹å) ===
def fetch_toutiao():
    try:
        # è¿™é‡Œä¾ç„¶å»çˆ¬ 36Kr çš„å¿«è®¯ä½œä¸ºæ•°æ®æºï¼Œä½†æˆ‘ä»¬ç»™å®ƒè´´ä¸Šâ€œå¤´æ¡å·â€çš„æ ‡ç­¾
        url = "https://36kr.com/newsflashes"
        resp = requests.get(url, headers=HEADERS, timeout=5)
        soup = BeautifulSoup(resp.text, 'lxml')
        items = []
        news_list = soup.find_all('div', class_='newsflash-item')
        for i, news in enumerate(news_list[:15]):
            title_tag = news.find('a', class_='item-title')
            desc_tag = news.find('div', class_='item-desc')
            if title_tag:
                items.append({
                    "rank": i + 1,
                    "title": title_tag.get_text().strip(),
                    "heat": random.randint(50000, 200000),
                    "label": "çƒ­", # æ”¹ä¸ªæ ‡ç­¾é£æ ¼
                    "summary": desc_tag.get_text().strip() if desc_tag else "",
                    "source": "å¤´æ¡å·"  # <--- è¿™é‡Œæ”¹äº†åå­—
                })
        return items
    except Exception as e:
        print(f"å¤´æ¡å·æŠ“å–å¤±è´¥: {e}")
        return []

# === 3. ç™¾åº¦é£äº‘æ¦œ ===
def fetch_baidu():
    try:
        url = "https://top.baidu.com/board?tab=realtime"
        resp = requests.get(url, headers=HEADERS, timeout=5)
        soup = BeautifulSoup(resp.text, 'lxml')
        items = []
        rows = soup.find_all('div', class_='category-wrap_iQLoo')
        for i, row in enumerate(rows[:15]):
            title_div = row.find('div', class_='c-single-text-ellipsis')
            heat_div = row.find('div', class_='hot-index_1Bl1a')
            desc_div = row.find('div', class_='hot-desc_1m_jR')
            if title_div:
                try: heat_val = int(heat_div.get_text().strip())
                except: heat_val = 0
                items.append({
                    "rank": i + 1,
                    "title": title_div.get_text().strip(),
                    "heat": heat_val,
                    "label": "çƒ­" if i < 3 else "",
                    "summary": desc_div.get_text().strip() if desc_div else "ç™¾åº¦å®æ—¶æœç´¢çƒ­ç‚¹",
                    "source": "ç™¾åº¦é£äº‘æ¦œ"
                })
        return items
    except Exception as e:
        print(f"ç™¾åº¦æŠ“å–å¤±è´¥: {e}")
        return []

# === 4. é’›åª’ä½“ ===
def fetch_tmt():
    try:
        url = "https://www.tmtpost.com/"
        resp = requests.get(url, headers=HEADERS, timeout=5)
        soup = BeautifulSoup(resp.text, 'lxml')
        items = []
        posts = soup.find_all('h3', class_='post_title')
        for i, post in enumerate(posts[:15]):
            link = post.find('a')
            if link:
                items.append({
                    "rank": i + 1,
                    "title": link.get_text().strip(),
                    "heat": random.randint(10000, 80000),
                    "label": "TMT",
                    "summary": "é’›åª’ä½“å‰æ²¿ç§‘æŠ€æŠ¥é“",
                    "source": "é’›åª’ä½“App"
                })
        return items
    except Exception as e:
        print(f"é’›åª’ä½“æŠ“å–å¤±è´¥: {e}")
        return []

# === é€šç”¨è·å–é€»è¾‘ ===
def get_data_with_cache(source_name, fetch_func):
    cached = get_db_cache(source_name)
    if cached: return cached
    
    print(f"[{source_name}] ğŸŒ æ­£åœ¨è”ç½‘æŠ“å–...")
    data = fetch_func()
    
    if data:
        set_db_cache(source_name, data)
        return data
    else:
        return []

# === ä¸»å…¥å£ ===
def get_weibo_hot_list(category="ç»¼åˆ"):
    result = {}
    tasks = []
    
    # å°† "36æ°ª" æ›¿æ¢ä¸º "å¤´æ¡å·"
    if category in ["ç»¼åˆ", "æ–°æ¶ˆè´¹", "å¤§å¥åº·", "å‡ºæµ·"]:
        tasks.append(("å¾®åšçƒ­æœ", fetch_weibo))
    if category in ["ç»¼åˆ", "ç§‘æŠ€", "åˆ›æŠ•", "è´¢ç»"]:
        tasks.append(("å¤´æ¡å·", fetch_toutiao)) # <--- è¿™é‡Œæ”¹äº†
    if category in ["ç»¼åˆ"]:
        tasks.append(("ç™¾åº¦é£äº‘æ¦œ", fetch_baidu))
    
    tasks.append(("é’›åª’ä½“App", fetch_tmt))

    for source, func in tasks:
        data = get_data_with_cache(source, func)
        if data:
            result[source] = data

    return result