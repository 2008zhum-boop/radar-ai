import sqlite3
import json
import time
import random
import re
import ai_engine

DB_FILE = "radar_data.db"

# === 1. åˆå§‹åŒ–ç›‘æ§ä¸“ç”¨è¡¨ (Database Schema) ===
def init_monitor_db():
    conn = sqlite3.connect(DB_FILE)
    c = conn.cursor()
    
    # 1.1 å®¢æˆ·é…ç½®è¡¨ (Client Config) - æ ¸å¿ƒé€»è¾‘å­˜å‚¨
    # monitor_logic: JSON structure including brand_keywords, exclude_keywords, advanced_rules
    c.execute('''CREATE TABLE IF NOT EXISTS client_config
                 (client_id VARCHAR(64) PRIMARY KEY,
                  name VARCHAR(100),
                  monitor_logic JSON,
                  risk_sensitivity FLOAT DEFAULT 1.0,
                  alert_webhook VARCHAR(255),
                  competitors JSON)''')
                  
    # 1.2 èˆ†æƒ…æ•°æ®è¡¨ (Mentions) - å­˜å‚¨å‘½ä¸­ç»“æœ
    # risk_level: 0=Safe, 1=Attention, 2=Warning, 3=Critical
    c.execute('''CREATE TABLE IF NOT EXISTS mentions
                 (id INTEGER PRIMARY KEY AUTOINCREMENT,
                  client_id VARCHAR(64),
                  source TEXT,
                  title TEXT,
                  content_text TEXT,
                  url TEXT,
                  publish_time REAL,
                  sentiment_score REAL,
                  risk_level INTEGER,
                  match_detail JSON,
                  FOREIGN KEY(client_id) REFERENCES client_config(client_id))''')
                  
    # Pre-populate with Demo Data if empty
    c.execute("SELECT count(*) FROM client_config")
    if c.fetchone()[0] == 0:
        demo_clients = [
            ("CLI_1001", "æ˜Ÿäº‘ç§‘æŠ€", json.dumps({
                "brand_keywords": ["æ˜Ÿäº‘ç§‘æŠ€", "Nebula", "N-Bot"],
                "exclude_keywords": ["æ˜Ÿäº‘æ³•å¸ˆ", "æ˜Ÿäº‘é”é“¾"],
                "advanced_rules": [
                    {"rule_name": "é«˜ç®¡è´Ÿé¢", "must_contain": ["å¼ ä¸‰", "CEO"], "nearby_words": ["é€ å‡", "è¢«æŠ“", "ç¦»èŒ"], "distance": 50}
                ]
            }), 1.0, ""),
            ("CLI_2002", "é›·å†›", json.dumps({
                "brand_keywords": ["é›·å†›", "é›·æ€»"],
                "exclude_keywords": [],
                "advanced_rules": []
            }), 1.2, "")
        ]
        c.executemany("INSERT INTO client_config (client_id, name, monitor_logic, risk_sensitivity, alert_webhook) VALUES (?,?,?,?,?)", demo_clients)
        conn.commit()
        
    conn.commit()
    conn.close()

init_monitor_db()

# === 2. æ ¸å¿ƒåŒ¹é…é€»è¾‘ (Matching Logic) ===

def check_advanced_rule(text, rule):
    """
    æ£€æŸ¥é«˜çº§è§„åˆ™: must_contain AND (nearby_words within distance)
    NOTE: ç®€å•å®ç° distanceï¼Œæš‚ä¸ä½¿ç”¨å¤æ‚çš„ NLP åˆ†è¯ï¼Œä»…ç”¨å­—ç¬¦è·ç¦»ä¼°ç®—
    """
    # 1. Check must_contain
    for word in rule.get('must_contain', []):
        if word not in text:
            return False, None
            
    # 2. Check nearby_words
    nearby_hits = []
    text_len = len(text)
    
    # æ‰¾åˆ°æ‰€æœ‰ must_contain è¯çš„ä½ç½®ï¼Œç„¶åå‘å‰åæœç´¢ nearby_words
    # ç®€åŒ–ç‰ˆï¼šåªè¦å…¨æ–‡åŒæ—¶åŒ…å« must_contain å’Œ nearby_wordsï¼Œä¸”ç²—ç•¥åˆ¤æ–­è·ç¦»
    for nearby in rule.get('nearby_words', []):
        if nearby in text:
            nearby_hits.append(nearby)
            
    if not nearby_hits:
        return False, None
        
    return True, nearby_hits

def match_client_logic(text, logic_config):
    """
    å°†æ–‡æœ¬ä¸å®¢æˆ·é€»è¾‘è¿›è¡ŒåŒ¹é…
    Return: { "matched": True/False, "type": "brand/advanced", "details": ... }
    """
    logic = logic_config if isinstance(logic_config, dict) else json.loads(logic_config)
    
    # 1. Exclusion Check (High Priority)
    for excl in logic.get('exclude_keywords', []):
        if excl in text:
            return None # Excluded
            
    # 2. Brand Keyword Match
    matched_brand = None
    for brand in logic.get('brand_keywords', []):
        if brand in text:
            matched_brand = brand
            break
            
    # 3. Advanced Rules Match
    advanced_hit = None
    for rule in logic.get('advanced_rules', []):
        is_hit, hit_words = check_advanced_rule(text, rule)
        if is_hit:
            advanced_hit = {"rule": rule['rule_name'], "words": hit_words}
            break
            
    if matched_brand or advanced_hit:
        return {
            "matched_brand": matched_brand,
            "advanced_hit": advanced_hit
        }
        
    return None

# === 3. é£é™©è¯„ä¼°ä¸AIåˆ†æ (Risk Assessment) ===

def get_source_weight(source_name):
    if source_name in ["å¾®åšçƒ­æœ", "å¤®è§†æ–°é—»", "äººæ°‘æ—¥æŠ¥", "è´¢è”ç¤¾"]:
        return 100
    if source_name in ["36æ°ª", "è™å—…", "é’›åª’ä½“", "å¤´æ¡å·"]:
        return 80
    return 50

def analyze_risk(text, match_result, source_weight, sentiment_score):
    """
    æ ¹æ®åŒ¹é…è¯¦æƒ…å’Œæƒ…æ„Ÿåˆ†ï¼Œåˆ¤å®šé£é™©ç­‰çº§
    Level: 0(Safe), 1(Info), 2(Warning), 3(Critical)
    """
    # 1. AI æ•æ„Ÿè¯æ£€æµ‹ (Simulation for now, call AI engine in real scenario)
    # ai_res = ai_engine.analyze_risk_assessment(text, match_result.get('matched_brand') or "General")
    # sensitive_hit = ai_res['risk_keywords']
    sensitive_words = ["çˆ†ç‚¸", "ç»´æƒ", "èµ·è¯‰", "é€ å‡", "ç ´äº§", "å»ä¸–"]
    hit_sensitive = [w for w in sensitive_words if w in text]
    
    # 2. é€»è¾‘åˆ¤å®š
    # ğŸ”´ Level 3: å‘½ä¸­é«˜çº§è´Ÿé¢è§„åˆ™ OR (ä¸¥é‡è´Ÿé¢ && (æƒé‡é«˜ OR å‘½ä¸­æ•æ„Ÿè¯))
    if match_result.get('advanced_hit'):
        return 3, "å‘½ä¸­é«˜çº§é£é™©è§„åˆ™: " + match_result['advanced_hit']['rule']
        
    if (sentiment_score < -0.4 and (source_weight >= 80 or hit_sensitive)):
        return 3, f"é«˜å±è´Ÿé¢ä¸”æƒé‡é«˜/æ•æ„Ÿ (å¾—åˆ†:{sentiment_score})"
        
    # ğŸŸ¡ Level 2: è´Ÿé¢æƒ…æ„Ÿ OR å‘½ä¸­æ•æ„Ÿè¯
    if sentiment_score < -0.2 or hit_sensitive:
        return 2, "ç–‘ä¼¼è´Ÿé¢é£é™©"
        
    # ğŸŸ¢ Level 1: æ™®é€šæåŠ
    return 1, "å¸¸è§„æåŠ"

# === 4. ä¸»å¤„ç†æµç¨‹ (Main Pipeline) ===

def process_monitor_data(raw_items):
    conn = sqlite3.connect(DB_FILE)
    c = conn.cursor()
    
    # Load all clients
    c.execute("SELECT client_id, monitor_logic, name FROM client_config")
    clients = c.fetchall()
    
    processed_count = 0
    alerts = []
    
    for item in raw_items:
        text = item['title'] + (item.get('summary') or "")
        source = item['source']
        weight = get_source_weight(source)
        
        # Call AI for sentiment once per item (optimization)
        # Note: In production, passing client context to AI is better, 
        # but for efficiency we get a general sentiment first.
        # Here we use a mockup or call ai_engine if needed.
        # ai_analysis = ai_engine.analyze_sentiment(text) 
        # For demo, using random or heuristic
        sentiment = -0.5 if "ç»´æƒ" in text else 0.5 
        if "å‘å¸ƒ" in text: sentiment = 0.8
        
        for client_row in clients:
            c_id, c_logic_json, c_name = client_row
            
            match_res = match_client_logic(text, c_logic_json)
            
            if match_res:
                # Determine Risk
                risk_level, reason = analyze_risk(text, match_res, weight, sentiment)
                
                # Insert Record
                c.execute('''INSERT INTO mentions 
                             (client_id, source, title, content_text, url, publish_time, 
                              sentiment_score, risk_level, match_detail)
                             VALUES (?,?,?,?,?,?,?,?,?)''',
                          (c_id, source, item['title'], text, item['url'], time.time(),
                           sentiment, risk_level, json.dumps({"reason": reason, "match": match_res})))
                           
                processed_count += 1
                
                if risk_level >= 2:
                    alerts.append({
                        "client": c_name,
                        "level": risk_level,
                        "title": item['title'],
                        "reason": reason
                    })
                    
    conn.commit()
    conn.close()
    return {"processed": processed_count, "alerts": alerts}

# === 5. API Support ===

def add_client_config(name, logic_dict, webhook=""):
    conn = sqlite3.connect(DB_FILE)
    c = conn.cursor()
    client_id = f"CLI_{int(time.time())}"
    c.execute("INSERT INTO client_config (client_id, name, monitor_logic, alert_webhook) VALUES (?,?,?,?)",
              (client_id, name, json.dumps(logic_dict), webhook))
    conn.commit()
    conn.close()
    return client_id

def get_monitor_stats():
    conn = sqlite3.connect(DB_FILE)
    c = conn.cursor()
    
    # Today's Mentions
    c.execute("SELECT count(*) FROM mentions WHERE publish_time > ?", (time.time() - 86400,))
    today_count = c.fetchone()[0]
    
    # Risk Count
    c.execute("SELECT count(*) FROM mentions WHERE risk_level >= 2 AND publish_time > ?", (time.time() - 86400,))
    risk_count = c.fetchone()[0]
    
    # Recent Logs
    logs = []
    c.execute('''SELECT m.source, m.title, m.risk_level, c.name, m.match_detail, m.publish_time 
                 FROM mentions m 
                 JOIN client_config c ON m.client_id = c.client_id 
                 ORDER BY m.id DESC LIMIT 20''')
    for row in c.fetchall():
        detail = json.loads(row[4])
        logs.append({
            "source": row[0],
            "title": row[1],
            "risk_level": row[2],
            "client_name": row[3],
            "reason": detail.get('reason', ''),
            "time": time.strftime("%H:%M", time.localtime(row[5]))
        })
        
    conn.close()
    return {
        "today_count": today_count,
        "risk_count": risk_count,
        "logs": logs
    }